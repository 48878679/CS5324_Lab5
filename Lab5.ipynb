{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4f9ba6",
   "metadata": {},
   "source": [
    "## Lab 5, Group 1\n",
    "### Names: Hailey DeMark, Deborah Park, Karis Park\n",
    "### Student IDs: 48869449, 48878679, 48563429\n",
    "\n",
    "Link to DataSet: (link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa554f44",
   "metadata": {},
   "source": [
    "## Preparation (4 points total)\n",
    "* [1 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). You have the option of using tf.dataset for processing, but it is not required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87ded9",
   "metadata": {},
   "source": [
    "* [1 points] Identify groups of features in your data that should be combined into cross-product features. Provide a compelling justification for why these features should be crossed (or why some features should not be crossed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9983dd7",
   "metadata": {},
   "source": [
    "* [1 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fe8d3",
   "metadata": {},
   "source": [
    "* [1 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Argue why your cross validation method is a realistic mirroring of how an algorithm would be used in practice. Use the method to split your data that you argue for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce731d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import FeatureSpace\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee2657",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff042c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\"BMI\", \"GenHlth\", \"MentHlth\", \"PhysHlth\", \"Age\", \"Education\", \"Income\"]\n",
    "outcome = \"Diabetes_binary\"\n",
    "categorical_cols = [ col for col in df.columns if col != outcome and col not in numeric_cols ]\n",
    "\n",
    "df[categorical_cols] = df[categorical_cols].astype(int).astype(str)\n",
    "df[outcome] = df[outcome].astype(int)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193f7a3",
   "metadata": {},
   "source": [
    "1 train/test split because we have a ton of data (switch to 5/10 fold if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359d7d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = copy.deepcopy(df)\n",
    "y = X.pop(outcome).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state=1234, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0bced",
   "metadata": {},
   "source": [
    "\n",
    "cross features \n",
    "- high BP \n",
    "- high cholesterol \n",
    "____ \n",
    "\n",
    "- stroke \n",
    "- deartdiseaseorAttack\n",
    "____ \n",
    "- smoker \n",
    "- hvyAlcoholConsump\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac678ac0",
   "metadata": {},
   "source": [
    "## Modeling (5 points total)\n",
    "* [2 points] Create at least three combined wide and deep networks to classify your data using Keras (this total of \"three\" includes the model you will train in the next step of the rubric). Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations.\n",
    "    * Note: you can use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a3340",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# need to redefine the tf dataset to include cataegorical variables\n",
    "# create a tensorflow dataset, for ease of use later\n",
    "batch_size = 64\n",
    "\n",
    "def create_dataset_from_dataframe(X, y):\n",
    "\n",
    "    # get numeric feature data to start with, with categorical_headers\n",
    "    df = {key: value.values[:,np.newaxis] for key, value in X[numeric_cols+categorical_cols].items()}\n",
    "\n",
    "    # create the Dataset here\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), y))\n",
    "    \n",
    "    # now enable batching and prefetching\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "ds_train = create_dataset_from_dataframe(X_train, y_train)\n",
    "ds_test = create_dataset_from_dataframe(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c4ea2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "for col in categorical_cols:\n",
    "  features[col] = FeatureSpace.string_categorical(num_oov_indices=0)\n",
    "for col in numeric_cols:\n",
    "  features[col] = FeatureSpace.float_normalized()\n",
    "\n",
    "feature_space = FeatureSpace(\n",
    "    features=features,\n",
    "    crosses=[\n",
    "        FeatureSpace.cross(\n",
    "          feature_names=('HighBP', 'HighChol'),\n",
    "          crossing_dim=2*2\n",
    "        ),\n",
    "        FeatureSpace.cross(\n",
    "          feature_names=('Stroke', 'HeartDiseaseorAttack'),\n",
    "          crossing_dim=2*2\n",
    "        ),\n",
    "        FeatureSpace.cross(\n",
    "          feature_names=('Smoker', 'HvyAlcoholConsump'),\n",
    "          crossing_dim=2*2\n",
    "        ),\n",
    "    ],\n",
    "    output_mode=\"concat\",\n",
    ")\n",
    "\n",
    "\n",
    "feature_space.adapt(ds_train.map(lambda x, _: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b67e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def setup_embedding_from_categorical(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    # which is the same as the number of categories\n",
    "    N = len(feature_space.preprocessors[col_name].get_vocabulary())\n",
    "    \n",
    "    # get the output from the feature space, which is input to embedding\n",
    "    x = feature_space.preprocessors[col_name].output\n",
    "    \n",
    "    # now use an embedding to deal with integers from feature space\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "    \n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "    \n",
    "    return x # return the tensor here \n",
    "\n",
    "def setup_embedding_from_crossing(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    \n",
    "    # get the size of the feature\n",
    "    N = feature_space.crossers[col_name].num_bins\n",
    "    x = feature_space.crossers[col_name].output\n",
    "    \n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "    \n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.title('Training')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Validation')\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.xlabel('epochs')\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.xlabel('epochs')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = \"loss\",\n",
    "    verbose = 1,\n",
    "    patience = 5,\n",
    "    mode = \"min\",\n",
    "    min_delta = 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87fa63",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af919ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dict_inputs = feature_space.get_inputs() # need to use unprocessed features here, to gain access to each output\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# for each crossed variable, make an embedding\n",
    "for col in feature_space.crossers.keys():\n",
    "    \n",
    "    x = setup_embedding_from_crossing(feature_space, col)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# for each numeric variable, just add it in after embedding\n",
    "for idx,col in enumerate(numeric_cols):\n",
    "    x = feature_space.preprocessors[col].output\n",
    "    #x = tf.cast(x,float) # cast an integer as a float here\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# for each categorical variable\n",
    "for col in categorical_cols:\n",
    "    \n",
    "    # get the output tensor from ebedding layer\n",
    "    x = setup_embedding_from_categorical(feature_space, col)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = Concatenate(name='embed_concat')(all_deep_branch_outputs)\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model1 = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "model1.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9c6ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "history1 = model1.fit(\n",
    "    ds_train, epochs=100, validation_data=ds_test, callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f5f40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bef2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(deep_neurons = [], final_neurons = []):\n",
    "  dict_inputs = feature_space.get_inputs() # need to use unprocessed features here, to gain access to each output\n",
    "\n",
    "  # we need to create separate lists for each branch\n",
    "  crossed_outputs = []\n",
    "\n",
    "  # for each crossed variable, make an embedding\n",
    "  for col in feature_space.crossers.keys():\n",
    "      \n",
    "      x = setup_embedding_from_crossing(feature_space, col)\n",
    "      \n",
    "      # save these outputs in list to concatenate later\n",
    "      crossed_outputs.append(x)\n",
    "      \n",
    "\n",
    "  # now concatenate the outputs and add a fully connected layer\n",
    "  wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "  # reset this input branch\n",
    "  all_deep_branch_outputs = []\n",
    "\n",
    "  # for each numeric variable, just add it in after embedding\n",
    "  for idx,col in enumerate(numeric_cols):\n",
    "      x = feature_space.preprocessors[col].output\n",
    "      #x = tf.cast(x,float) # cast an integer as a float here\n",
    "      all_deep_branch_outputs.append(x)\n",
    "      \n",
    "  # for each categorical variable\n",
    "  for col in categorical_cols:\n",
    "      \n",
    "      # get the output tensor from ebedding layer\n",
    "      x = setup_embedding_from_categorical(feature_space, col)\n",
    "      \n",
    "      # save these outputs in list to concatenate later\n",
    "      all_deep_branch_outputs.append(x)\n",
    "\n",
    "\n",
    "  # merge the deep branches together\n",
    "  deep_branch = Concatenate(name='embed_concat')(all_deep_branch_outputs)\n",
    "  for i, neurons in enumerate(deep_neurons):\n",
    "      deep_branch = Dense(units=neurons,activation='relu', name=f'deep{i}')(deep_branch)\n",
    "      \n",
    "  # merge the deep and wide branch\n",
    "  final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "  for i, neurons in enumerate(final_neurons):\n",
    "      final_branch = Dense(units=neurons,activation='relu', name=f'final{i}')(final_branch)\n",
    "  final_branch = Dense(units=1,activation='sigmoid',\n",
    "                      name='combined')(final_branch)\n",
    "\n",
    "  model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "  model.compile(\n",
    "      optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  print(model.summary())\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43a7e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model1 = build_model([50, 25, 10], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b775885",
   "metadata": {},
   "source": [
    "* [2 points] Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two models (this \"two\" includes the wide and deep model trained from the previous step). Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to answer: What model with what number of layers performs superiorly? Use proper statistical methods to compare the performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d68dc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03e55b1",
   "metadata": {},
   "source": [
    "* [1 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). For classification tasks, compare using the receiver operating characteristic and area under the curve. For regression tasks, use Bland-Altman plots and residual variance calculations.  Use proper statistical methods to compare the performance of different models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b35b07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc7aa17",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41a5eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
